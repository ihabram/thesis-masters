{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on this notebook: https://www.kaggle.com/code/eriknovak/pytorch-roberta-named-entity-recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualization libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# pytorch libraries\n",
    "import torch # the main pytorch library\n",
    "import torch.nn as nn # the sub-library containing Softmax, Module and other useful functions\n",
    "import torch.optim as optim # the sub-library containing the common optimizers (SGD, Adam, etc.)\n",
    "\n",
    "# huggingface's transformers library\n",
    "from transformers import RobertaForTokenClassification, RobertaTokenizer\n",
    "\n",
    "# huggingface's datasets library\n",
    "from datasets import load_dataset\n",
    "\n",
    "# the tqdm library used to show the iteration progress\n",
    "import tqdm\n",
    "tqdmn = tqdm.notebook.tqdm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import RoBERTa tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roberta_version = 'roberta-base'\n",
    "tokenizer = RobertaTokenizer.from_pretrained(roberta_version)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"conll2003\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_labels = dataset['train'].features['ner_tags'].feature.num_classes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_encodings(example):\n",
    "    \"\"\"Processing the example\n",
    "    \n",
    "    Args:\n",
    "        example (dict): The dataset example.\n",
    "    \n",
    "    Returns:\n",
    "        dict: The dictionary containing the following updates:\n",
    "            - input_ids: The list of input ids of the tokens.\n",
    "            - attention_mask: The attention mask list.\n",
    "            - ner_tags: The updated ner_tags.\n",
    "    \n",
    "    \"\"\"\n",
    "    # get the encodings of the tokens. The tokens are already split, that is why we must add is_split_into_words=True\n",
    "    encodings = tokenizer(example['tokens'], truncation=True, padding='max_length', is_split_into_words=True)\n",
    "    # extend the ner_tags so that it matches the max_length of the input_ids\n",
    "    labels = example['ner_tags'] + [0] * (tokenizer.model_max_length - len(example['ner_tags']))\n",
    "    # return the encodings and the extended ner_tags\n",
    "    return { **encodings, 'labels': labels }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modify/format all datasets so that they include the 'input_ids', 'attention_mask' \n",
    "# and 'labels' used to train and evaluate the model\n",
    "dataset = dataset.map(add_encodings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# format the datasets so that we return only 'input_ids', 'attention_mask' and 'labels' \n",
    "# making it easier to train and validate the model\n",
    "dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the NER labels and create two dictionaries for accessing their ids\n",
    "labels = dataset['train'].features['ner_tags'].feature\n",
    "label2id = { k: labels.str2int(k) for k in labels.names }\n",
    "id2label = { v: k for k, v in label2id.items() }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the model and provide the 'num_labels' used to create the classification layer\n",
    "model = RobertaForTokenClassification.from_pretrained(roberta_version, num_labels=num_labels)\n",
    "# assign the 'id2label' and 'label2id' model configs\n",
    "model.config.id2label = id2label\n",
    "model.config.label2id = label2id"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the model in 'train' mode and send it to the device\n",
    "model.train().to(device)\n",
    "# initialize the Adam optimizer (used for training/updating the model)\n",
    "optimizer = optim.AdamW(params=model.parameters(), lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the number of epochs \n",
    "n_epochs = 3\n",
    "# batch the train data so that each batch contains 4 examples (using 'batch_size')\n",
    "train_data = torch.utils.data.DataLoader(dataset['train'], batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = []\n",
    "# iterate through the data 'n_epochs' times\n",
    "for epoch in tqdmn(range(n_epochs)):\n",
    "    current_loss = 0\n",
    "    # iterate through each batch of the train data\n",
    "    for i, batch in enumerate(tqdmn(train_data)):\n",
    "        # move the batch tensors to the same device as the \n",
    "        batch = { k: v.to(device) for k, v in batch.items() }\n",
    "        # send 'input_ids', 'attention_mask' and 'labels' to the model\n",
    "        outputs = model(**batch)\n",
    "        # the outputs are of shape (loss, logits)\n",
    "        loss = outputs[0]\n",
    "        # with the .backward method it calculates all \n",
    "        # of  the gradients used for autograd\n",
    "        loss.backward()\n",
    "        # NOTE: if we append `loss` (a tensor) we will force the GPU to save\n",
    "        # the loss into its memory, potentially filling it up. To avoid this\n",
    "        # we rather store its float value, which can be accessed through the\n",
    "        # `.item` method\n",
    "        current_loss += loss.item()\n",
    "        if i % 8 == 0 and i > 0:\n",
    "            # update the model using the optimizer\n",
    "            optimizer.step()\n",
    "            # once we update the model we set the gradients to zero\n",
    "            optimizer.zero_grad()\n",
    "            # store the loss value for visualization\n",
    "            train_loss.append(current_loss / 32)\n",
    "            current_loss = 0\n",
    "    # update the model one last time for this epoch\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RoBERTa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
